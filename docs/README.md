# Paper Notes {docsify-ignore-all}

本仓库用来记录论文笔记和总结，阅读时将作者观点和结论记录为自己的笔记，是一种对文献的理解和提炼过程，这有助于更好地理解和记忆文献。随着时间的推移，
这一实践将对文献的理解更加娴熟，犹如庖丁解牛一般。好记性不如烂笔头，读过的论文过一段时间总是会忘记。当需要再次读这篇文献的时候，只要读笔记即可，而不是再把几十页的PDF再翻一遍。



## 环境安装
### Node.js版本
Node v16

### 安装docsify
```shell
npm i docsify-cli -g
```


### 启动docsify
```shell
docsify serve ./docs
```

## BASE



## NLP

| 名字 | 简介 | 地址 | 会议/期刊 | 年份 | 笔记 | 代码 |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
|   Improving Language Understanding by Generative Pre-Training   |   GPT1   | [paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)   | arXiv |  2018  |  [note](./contents/NLP/Improving%20Language%20Understanding%20by%20Generative%20Pre-Training.md)  |  |   
|   Language Models are Unsupervised Multitask Learners   |   GPT2   |   [paper](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf)   |  arXiv |  2019 |   [note](./contents/NLP/Language%20Models%20are%20Unsupervised%20Multitask%20Learners.md)   |      |  
|    Language Models are Few-Shot Learners   |   GPT3   |   [paper](https://arxiv.org/abs/2005.14165)   |  arXiv |  2020 |   [note](./contents/NLP/Language%20Models%20are%20Few-Shot%20Learners.md)   |      |  
|     BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding   |   BERT   |   [paper](https://arxiv.org/abs/1810.04805)   |  NAACL |  2018 |   [note](./contents/NLP/BERT.md)   |   [code](https://github.com/codertimo/BERT-pytorch)  |  
|     XLNet: Generalized Autoregressive Pretraining for Language Understanding  |   XLNet   |   [paper](https://arxiv.org/abs/1906.08237)   |  NeurIPS |  2019 |   [note](./contents/NLP/XLNet.md)   |   [code](https://github.com/zihangdai/xlnet)  |



## CV


| 名字 | 简介 | 地址 | 会议/期刊 | 年份 | 笔记 | 代码 | 
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
|      |      |      |      |      |      |      | 


## LLM

| 名字 | 简介 | 地址 | 会议/期刊 | 年份 | 笔记 | 代码 | 
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
|      |      |     |     |      |      |      | 


## Model Compression

| 名字 | 简介 | 地址 | 会议/期刊 | 年份 | 笔记 | 代码 | 
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
|   DepGraph: Towards Any Structural Pruning   |      |  [paper](https://arxiv.org/abs/2301.12900)   |  CVPR   |  2023    |  [note](./contents/ModelCompression/DepGraph.md)    |  [code](https://github.com/VainF/Torch-Pruning)    | 