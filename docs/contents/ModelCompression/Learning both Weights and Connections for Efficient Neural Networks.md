# Learning both Weights and Connections for Efficient Neural Networks

**作者** Song Han,Jeff Pool,John Tran,William J. Dally

**发表刊物/会议：** 

**发表年份：** 2015

**论文地址：** https://arxiv.org/pdf/1506.02626.pdf

**代码地址：** https://github.com/jack-willturner/deep-compression

## 内容概要

这篇论文提出了一种通过学习神经网络中重要连接的方法来提高神经网络的能效和存储效率，同时不影响其准确性。作者提出了一种三阶段方法来减少神经网络的存储和计算需求，使其降低一个数量级。首先，通过训练网络来学习哪些连接是重要的；接着，剪除不重要的连接；最后，重新训练网络以调整剩余连接的权重。在ImageNet数据集上，作者的方法将AlexNet的参数数量减少了9倍，从6100万减少到670万，而没有损失准确性。类似的实验在VGG-16上发现，总参数数量可以减少13倍，从1380万减少到1030万，同样没有损失准确性。

## 主要解决问题/应用

如何在不损失精度的前提下，对DNN进行剪枝（或者说稀疏化），从而压缩模型。

这种方法主要解决了神经网络在计算和存储方面的资源需求问题。由于神经网络具有大量的参数和连接，它们在计算和存储方面的需求很高，这使得它们难以部署在嵌入式系统上。此外，传统神经网络在训练开始前就固定了架构，因此训练过程无法改进架构。

通过学习神经网络中的重要连接并剪除不重要的连接，这种方法可以显著降低神经网络的计算和存储需求，同时保持其准确性。这使得神经网络可以在资源受限的设备（如移动设备）上实时运行，并有助于存储和传输包含深度神经网络的移动应用程序。

## 主要使用方法/模型

具体方法分为三个阶段：

1. 训练连接：首先，通过正常网络训练来学习连接。与传统训练不同，这里不是学习权重的最终值，而是学习哪些连接是重要的。

2. 剪除连接：在第一阶段学习到重要连接后，接下来是剪除低权重连接。所有权重低于阈值的连接将从网络中移除，将密集的全连接层转换为稀疏层。

3. 重新训练权重：最后一步是重新训练稀疏网络，使剩余连接能够补偿已移除的连接。剪除和重新训练阶段可以迭代进行，以进一步降低网络复杂性。

通过这种方法，训练过程实际上学会了网络的连接性以及权重，类似于哺乳动物大脑中突触的创建和逐渐剪除不常用连接的过程。

## 主要实验手段/数据集

作者在实验中使用了四个代表性的神经网络：LeNet-300-100和LeNet-5在MNIST数据集上，以及AlexNet和VGG-16在ImageNet数据集上。这些网络在各自的任务上具有较高的准确性，并且具有不同数量的参数和连接，因此可以很好地展示剪枝方法的效果。

在MNIST数据集上，LeNet-300-100和LeNet-5分别具有267K和431K个参数。在ImageNet数据集上，AlexNet和VGG-16分别具有61M和138M个参数。通过使用所提出的方法，作者在这些网络中实现了显著的参数压缩和计算需求降低，同时保持了原始的准确性。

## 创造性思考

这篇论文提出了一种创新的方法来解决神经网络在计算和存储方面的资源需求问题。

1. 将神经网络的剪枝过程与哺乳动物大脑中突触的创建和逐渐剪除不常用连接的过程相类比，为剪枝方法提供了生物学上的启示。

2. 通过迭代剪枝和重新训练过程，可以在保持准确性的同时进一步提高参数压缩率。

3. 在剪枝过程中，作者发现网络可以自动检测到视觉关注区域，这为进一步研究神经网络的可解释性和可视化提供了有趣的线索。

4. 通过将剪枝后的网络存储为稀疏矩阵，可以进一步降低存储需求和计算成本。


## 批判式思考

尽管这篇论文提出了一种创新的方法来解决神经网络在计算和存储方面的资源需求问题，但仍然存在一些可能潜在的问题和局限性：

1. 剪枝方法可能对某些类型的神经网络和任务效果不佳。例如，对于具有高度非线性和复杂结构的网络，剪枝可能导致性能下降。因此，在应用这种方法时需要仔细考虑网络结构和任务需求。

2. 剪枝方法可能需要大量的计算资源和时间。尽管剪枝可以降低网络的计算需求，但剪枝过程本身可能需要大量的计算资源和时间。这可能限制了剪枝方法在实时应用中的使用。

3. 剪枝方法可能对网络的稳定性产生负面影响。剪枝后的网络可能更容易受到噪声和扰动的影响，导致性能下降。为了解决这个问题，可能需要在剪枝过程中引入稳定性约束。


## 讨论 

1. 如何理解与传统训练不同的是，我们不是在学习权重的最终值，而是在学习哪些连接是重要的？这具体是怎么实现的？
2. 裁剪之后为什么不重新初始化修剪过的层，而是直接训练？
3. L1和L2正则化的区别是什么？分别适用于什么时候？






