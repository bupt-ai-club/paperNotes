# LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery


**作者** Tianyi Chen  Tianyu Ding  Badal Yadav  Ilya Zharkov  Luming Liang 

**发表刊物/会议：**

**发表年份：**

**论文地址：**

**代码地址：**

## 内容概要

这篇论文提出了一种名为LoRAShear的新颖方法，用于在有限资源条件下对大型语言模型（LLMs）进行高效结构化剪枝和知识恢复。LoRAShear具有以下三个主要特点：

1. 自动发现LLMs与LoRA模块中的最小可移除结构。
2. 通过一种名为LoRA Half-Space Projected Gradient（LHSPG）的新型结构稀疏优化器进行渐进式结构化剪枝，利用LoRA模块中存储的信息在原始变量上产生结构稀疏。
3. 配备动态知识恢复阶段，以便从预训练和有指导的微调数据集恢复知识。
   
实验结果表明，LoRAShear在20%的剪枝比例下，性能仅下降1%，而在50%的剪枝比例下，性能保持在82%。这表明LoRAShear在剪枝和知识恢复方面具有显著优势。


## 主要解决问题/应用

LoRAShear主要解决了大型语言模型（LLMs）在有限资源下进行结构化修剪和知识恢复的问题。由于LLMs通常具有数十亿到数百亿个参数，这导致了处理能力和内存需求的巨大计算成本

## 主要使用方法/模型


- 依赖图分析：通过构建依赖图来发现LLMs中可最小化移除的结构，并分析知识分布。这有助于确定哪些结构在修剪过程中对性能影响较小，从而可以安全地移除。

- LoRA Half-Space Projected Gradient (LHSPG)：这是一种新颖的结构稀疏优化算法，用于进行渐进式结构修剪。LHSPG利用LoRA模块的信息来更新权重，从而在修剪过程中更好地保留知识。

- 动态知识恢复：在修剪后，LoRAShear通过动态知识恢复阶段从预训练和有指导的微调数据集中恢复知识。这包括两个阶段：预训练阶段和有指导的微调阶段。预训练阶段通过动态构建数据集来恢复一般知识，而有指导的微调阶段则通过使用LoRA进行微调来恢复特定领域的知识和指令遵循能力。

通过这些方法和模型，LoRAShear能够在有限资源下对大型语言模型进行高效的结构化修剪和知识恢复，从而降低计算成本和内存需求，同时保持模型性能。

## 主要实验手段/数据集

 LoRAShear的主要实验手段和数据集如下：

1. 数据集选择：预训练数据集包括OpenWebText、Wikipedia、Gutenberg和BookCorpus。这些数据集用于评估模型在不同领域和任务上的性能。有指导的微调数据集使用Alpaca数据集，包含52,000个由OpenAI的text-davinci-003引擎生成的指令和演示。

2. 实验方法：LoRAShear首先自动发现LLMs中可最小化移除的结构，然后通过LHSPG进行渐进式结构化剪枝。接下来，LoRAShear通过动态知识恢复阶段从预训练和有指导的微调数据集中恢复知识。

3. 评估指标：实验结果通过在不同任务和数据集上评估模型性能来衡量。主要任务包括BoolQ、PIQA、HellaSwag、WinoGrande、ARC-e、ARC-c和OBQA等。

4. 比较方法：LoRAShear与其他剪枝方法进行了比较，如LLM-Pruner、LoRAPrune和WANDA。这些方法在有限资源下对LLMs进行剪枝，但性能相对较差。


## 创造性思考

LoRAShear提出了一种新颖的方法，用于在有限资源下对大型语言模型（LLMs）进行有效的结构化剪枝和知识恢复。这种方法的创造性思考体现在以下几个方面：

1. 自动发现可最小化移除的结构：LoRAShear通过依赖图分析自动发现LLMs中可最小化移除的结构。这使得剪枝过程更加高效且无需人工干预。

2. LoRA Half-Space Projected Gradient（LHSPG）：LoRAShear提出了一种新型结构稀疏优化算法LHSPG，利用LoRA模块中的信息来更新权重。这使得剪枝过程中的知识保留更加有效。

3. 动态知识恢复阶段：LoRAShear配备了动态知识恢复阶段，从预训练和有指导的微调数据集中恢复知识。这使得剪枝后的模型能够在性能上接近原始模型。

4. 通用性：LoRAShear适用于各种LLMs，而不仅仅是特定的模型。这使得LoRAShear具有广泛的适用性。

## 批判式思考

虽然LoRAShear在大型语言模型（LLMs）的剪枝和知识恢复方面取得了显著成果，但仍存在一些潜在的问题和局限性：

1. 计算资源需求：尽管LoRAShear在有限资源下进行了剪枝和知识恢复，但这种方法仍然需要大量的计算资源。对于资源有限的研究者和开发者来说，这可能是一个挑战。

2. 剪枝效果的可扩展性：LoRAShear在20%和50%的剪枝比下取得了较好的性能，但在更高的剪枝比下，性能可能会进一步下降。因此，LoRAShear在极高剪枝比下的性能仍需进一步研究。

3. 动态知识恢复阶段的复杂性：LoRAShear的动态知识恢复阶段涉及从预训练和有指导的微调数据集中恢复知识。这可能导致恢复过程变得复杂，需要更多的计算资源和时间。

4. 依赖特定数据集和模型：LoRAShear在某些数据集和模型上可能表现良好，但在其他数据集和模型上可能效果不佳。因此，LoRAShear的通用性可能受到限制。

5. 可解释性：LoRAShear的剪枝和知识恢复过程可能难以解释，这可能限制了其在某些应用场景中的使用。



## 讨论 


1. 建立依赖图的依赖是什么？具体是怎么做的？


2. 本文方法与其他大模型裁剪的区别是什么？
   

3. 




