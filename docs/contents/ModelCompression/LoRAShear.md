# LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery


**作者** Tianyi Chen  Tianyu Ding  Badal Yadav  Ilya Zharkov  Luming Liang 

**发表刊物/会议：** arXiv

**发表年份：** 2023

**论文地址：** https://arxiv.org/abs/2310.18356

**代码地址：** 

## 内容概要

这篇论文提出了一种名为LoRAShear的新颖方法，用于在有限资源条件下对大型语言模型（LLMs）进行高效结构化剪枝和知识恢复。LoRAShear具有以下三个主要特点：

1. 自动发现LLMs与LoRA模块中的最小可移除结构。
2. 通过一种名为LoRA Half-Space Projected Gradient（LHSPG）的新型结构稀疏优化器进行渐进式结构化剪枝，利用LoRA模块中存储的信息在原始变量上产生结构稀疏。
3. 配备动态知识恢复阶段，以便从预训练和有指导的微调数据集恢复知识。
   
实验结果表明，LoRAShear在20%的剪枝比例下，性能仅下降1%，而在50%的剪枝比例下，性能保持在82%。这表明LoRAShear在剪枝和知识恢复方面具有显著优势。


## 主要解决问题/应用

LoRAShear主要解决了大型语言模型（LLMs）在有限资源下进行结构化修剪和知识恢复的问题。由于LLMs通常具有数十亿到数百亿个参数，这导致了处理能力和内存需求的巨大计算成本

## 主要使用方法/模型


- 依赖图分析：通过构建依赖图来发现LLMs中可最小化移除的结构，并分析知识分布。这有助于确定哪些结构在修剪过程中对性能影响较小，从而可以安全地移除。

- LoRA Half-Space Projected Gradient (LHSPG)：这是一种新颖的结构稀疏优化算法，用于进行渐进式结构修剪。LHSPG利用LoRA模块的信息来更新权重，从而在修剪过程中更好地保留知识。

- 动态知识恢复：在修剪后，LoRAShear通过动态知识恢复阶段从预训练和有指导的微调数据集中恢复知识。这包括两个阶段：预训练阶段和有指导的微调阶段。预训练阶段通过动态构建数据集来恢复一般知识，而有指导的微调阶段则通过使用LoRA进行微调来恢复特定领域的知识和指令遵循能力。

通过这些方法和模型，LoRAShear能够在有限资源下对大型语言模型进行高效的结构化修剪和知识恢复，从而降低计算成本和内存需求，同时保持模型性能。

## 主要实验手段/数据集

 LoRAShear的主要实验手段和数据集如下：

1. 数据集选择：预训练数据集包括OpenWebText、Wikipedia、Gutenberg和BookCorpus。这些数据集用于评估模型在不同领域和任务上的性能。有指导的微调数据集使用Alpaca数据集，包含52,000个由OpenAI的text-davinci-003引擎生成的指令和演示。

2. 实验方法：LoRAShear首先自动发现LLMs中可最小化移除的结构，然后通过LHSPG进行渐进式结构化剪枝。接下来，LoRAShear通过动态知识恢复阶段从预训练和有指导的微调数据集中恢复知识。

3. 评估指标：实验结果通过在不同任务和数据集上评估模型性能来衡量。主要任务包括BoolQ、PIQA、HellaSwag、WinoGrande、ARC-e、ARC-c和OBQA等。

4. 比较方法：LoRAShear与其他剪枝方法进行了比较，如LLM-Pruner、LoRAPrune和WANDA。这些方法在有限资源下对LLMs进行剪枝，但性能相对较差。


## 创造性思考

LoRAShear提出了一种新颖的方法，用于在有限资源下对大型语言模型（LLMs）进行有效的结构化剪枝和知识恢复。这种方法的创造性思考体现在以下几个方面：

1. 自动发现可最小化移除的结构：LoRAShear通过依赖图分析自动发现LLMs中可最小化移除的结构。这使得剪枝过程更加高效且无需人工干预。

2. LoRA Half-Space Projected Gradient（LHSPG）：LoRAShear提出了一种新型结构稀疏优化算法LHSPG，利用LoRA模块中的信息来更新权重。这使得剪枝过程中的知识保留更加有效。

3. 动态知识恢复阶段：LoRAShear配备了动态知识恢复阶段，从预训练和有指导的微调数据集中恢复知识。这使得剪枝后的模型能够在性能上接近原始模型。

4. 通用性：LoRAShear适用于各种LLMs，而不仅仅是特定的模型。这使得LoRAShear具有广泛的适用性。

## 批判式思考

虽然LoRAShear在大型语言模型（LLMs）的剪枝和知识恢复方面取得了显著成果，但仍存在一些潜在的问题和局限性：

1. 计算资源需求：尽管LoRAShear在有限资源下进行了剪枝和知识恢复，但这种方法仍然需要大量的计算资源。对于资源有限的研究者和开发者来说，这可能是一个挑战。

2. 剪枝效果的可扩展性：LoRAShear在20%和50%的剪枝比下取得了较好的性能，但在更高的剪枝比下，性能可能会进一步下降。因此，LoRAShear在极高剪枝比下的性能仍需进一步研究。

3. 动态知识恢复阶段的复杂性：LoRAShear的动态知识恢复阶段涉及从预训练和有指导的微调数据集中恢复知识。这可能导致恢复过程变得复杂，需要更多的计算资源和时间。

4. 依赖特定数据集和模型：LoRAShear在某些数据集和模型上可能表现良好，但在其他数据集和模型上可能效果不佳。因此，LoRAShear的通用性可能受到限制。

5. 可解释性：LoRAShear的剪枝和知识恢复过程可能难以解释，这可能限制了其在某些应用场景中的使用。



## 讨论 


1. 建立依赖图的依据是什么？具体是怎么做的？
 建立依赖图的依据是分析模型的结构，以便发现可以最小化移除的结构。具体做法如下：
   - 首先，遍历模型的基本操作符（例如，线性变换、激活函数等），并将它们分为基本节点组（Nbasic）和组合节点组（Ncomposed）。基本节点组包含剩余的操作符，而组合节点组包含由多个基本操作符组成的操作符，例如LoRA模块，这些操作符需要被视为一个整体。
   -  接下来，建立基本节点组和组合节点组之间的依赖关系。这可以通过遍历模型的计算图（例如，执行图）并确定哪些操作符依赖于其他操作符来完成。
   -  然后，将模型的可训练变量划分为最小可移除结构组。每个组对应于依赖图中的一个节点组。这些组可以分为可移除组（Gprunable）和不可移除组（Gunprunable）。
   -  分析知识分布在每个节点组上，以确定哪些节点组在剪枝过程中不应受到影响。这可以通过测量剪枝后模型在评估数据集上的性能偏差来完成。具有最大偏差的节点组将被标记为不可移除组。

2. 本文方法与其他大模型裁剪的区别是什么？
    本文方法LoRAShear与其他大型模型剪枝方法的主要区别在于：
   - LoRAShear自动发现LLMs中可以最小化移除的结构，而其他方法通常依赖于手动选择或启发式方法来确定要移除的结构。
   - LoRAShear使用一种新颖的结构稀疏优化算法LoRA Half-Space Projected Gradient（LHSPG）进行渐进式结构剪枝。LHSPG利用LoRA模块的信息来更新权重，从而在剪枝过程中实现知识转移和知识保留。这与其他方法仅依赖于原始模型权重的剪枝方法不同。
   - LoRAShear具有一个动态知识恢复阶段，该阶段包括预训练和有指导的细调，以从预训练和有指导的细调数据集中恢复丢失的知识。这使得LoRAShear能够在剪枝后有效地恢复模型的性能，而其他方法通常仅依赖于有指导的细调来恢复性能。





