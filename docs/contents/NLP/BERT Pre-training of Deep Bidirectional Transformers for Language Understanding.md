# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

**作者** Jacob Devlin、Ming-Wei Chang、Kenton Lee、Kristina Toutanova等

**发表刊物/会议：** NAACL

**发表年份：** 2018

**论文地址：** https://arxiv.org/abs/1810.04805

**代码地址：** https://github.com/codertimo/BERT-pytorch

## 内容概要

本文引入了一种新的语言表示模型BERT，它由transformer的双向编码器组成。不像最近的语言表示模型，BERT旨在通过联合条件所有层中的上下文，预训练无标记文本的深度双向表示。只需要一个额外的输出层，就可以对预先训练的BERT模型进行微调，为广泛的任务创建最先进的模型，例如问题回答和语言推断，而无需对特定于任务的体系结构进行大量修改，在11个NLP任务上表现SOTA。

## 主要解决问题/应用

现有的技术严重制约了预训练表示的能力。其主要局限在于标准语言模型是单向的，这使得在模型的预训练中可以使用的架构类型很有限。

## 主要使用方法/模型

整体是一个自编码语言模型（Autoencoder LM），并且其设计了两个任务来预训练该模型。第一个任务是采用MaskLM的方式来训练语言模型，通俗地说就是在输入一句话的时候，随机地选一些要预测的词，然后用一个特殊的符号[MASK]来代替它们，之后让模型根据所给的标签去学习这些地方该填的词。第二个任务在双向语言模型的基础上额外增加了一个句子级别的连续性预测任务，即预测输入BERT的两段文本是否为连续的文本。

## 主要实验手段/数据集

在11个NLP任务上表现SOTA。

## 创造性思考

本文证明了双向预训练对语言表示的重要性。与之前使用的单向语言模型进行预训练不同，BERT使用遮蔽语言模型来实现预训练的深度双向表示。 论文表明，预先训练的表示免去了许多工程任务需要针对特定任务修改体系架构的需求。 BERT是第一个基于微调的表示模型，它在大量的句子级和token级任务上实现了最先进的性能，强于许多面向特定任务体系架构的系统。 BERT刷新了11项NLP任务的性能记录。本文还报告了 BERT 的模型简化研究（ablation study），表明模型的双向性是一项重要的新成果。相关代码和预先训练的模型将会公布在goo.gl/language/bert上。 BERT目前已经刷新的11项自然语言处理任务的最新记录包括：将GLUE基准推至80.4％（绝对改进7.6％），MultiNLI准确度达到86.7% （绝对改进率5.6%），将SQuAD v1.1问答测试F1得分纪录刷新为93.2分（绝对提升1.5分），超过人类表现2.0分。

## 批判式思考

BERT在第一个预训练阶段，假设句子中多个单词被Mask掉，这些被Mask掉的单词之间没有任何关系，是条件独立的，然而有时候这些单词之间是有关系的。
BERT的在预训练时会出现特殊的[MASK]，但是它在下游的fine-tune中不会出现，这就出现了预训练阶段和fine-tune阶段不一致的问题。


## 讨论 







