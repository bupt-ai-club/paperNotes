# Improving Language Understanding by Generative Pre-Training

**作者** Alec Radford、Karthik Narasimhan、Tim Salimans、Ilya Sutskever等

**发表刊物/会议：** arXiv

**发表年份：** 2018

**论文地址：** https://www.mikecaptain.com/resources/pdf/GPT-1.pdf

**代码地址：** 

## 内容概要

自然语言理解包括一系列不同的任务，如文本相关性、问题回答、语义相似性评估和文档分类。虽然大量的无标签文本语料库很丰富，但用于学习这些特定任务的有标签的数据却很少，这使得经过鉴别性训练的模型很难有充分的表现。本文证明，通过在不同的无标签文本语料库上对语言模型进行生成性预训练，然后在每个具体任务上进行鉴别性微调，可以在这些任务上取得巨大的收益。与以前的方法相比，本文在微调过程中利用了任务理解的输入转换来实现有效的转移，同时要求对模型结构进行最小的改变。本文在广泛的自然语言理解基准上证明了本文方法的有效性。本文的通用任务诊断模型优于使用专门为每个任务设计的架构的鉴别性训练模型，在所研究的12个任务中的9个任务中明显改善了技术水平。例如，本文在常识推理（Stories Cloze Test）方面取得了8.9%的绝对改进，在问题回答（RACE）方面取得了5.7%的绝对改进，在文本关联（MultiNLI）方面取得了1.5%的改进。


## 主要解决问题/应用

在超大的文本数据集上预训练一个通用的模型，接着再对下游的特定任务微调的PRETRAIN-FINETUNE。GPT更进一步减少了人工的参与。不再需要对于每个任务采取不同的模型架构，而是用一个取得了优异泛化能力的模型，去针对性地对下游任务进行微调。


## 主要使用方法/模型

使用Transformer的解码器，由于Mask的存在，只能看到前面的元素。训练程序包括两个阶段。第一阶段是在一个大型文本语料库上学习一个高容量的语言模型。第二个阶段是一个微调阶段，在这个阶段，使模型适应带有标记数据的判别性任务。Supervised fine-tuning就是在具体任务的标注数据上进行调优，以文本分类任务为例，只需要将最后1个token的最上层的激活向量再过一层线性转换+softmax。针对其他一些任务的结构化输入数据，提出使用遍历（travelsal-style approach）的方法，模型框架改动小，灵活方便。


## 主要实验手段/数据集

在所研究的12个任务中的9个任务中（如句子相似度、分类、问答、自然语言推测等）表现SOTA。

## 创造性思考

本文引入了一个框架，通过生成性的预训练和辨别性的微调，用一个单一的任务诊断模型实现强大的自然语言理解。不管下游任务输入及输出如何变化，模型的结构都不变。

## 批判式思考

主要贡献在将Transformer发扬光大（相比word-level的嵌入词向量能够学习到更丰富的语义语境信息，相比传统的RNN网络能够建模更长距离的相关信息），沿用Unsupervised pretraining + Supervised fine-tuning的套路，在很多任务上刷新了成绩。但创新性不强，在工业实践上提供了一定参考。

## 讨论 







