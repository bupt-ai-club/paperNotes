# Language Models are Few-Shot Learners

**作者** Tom B. Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah等

**发表刊物/会议：** arXiv

**发表年份：** 2020

**论文地址：** https://arxiv.org/abs/2005.14165

**代码地址：**

## 内容概要

最近的工作表明，在许多NLP任务和基准上，通过对大型文本语料库进行预训练，然后对特定的任务进行微调，可以获得巨大的收益。虽然在结构上通常是任务无关的，但这种方法仍然需要特定任务的微调数据集，包括几千或几万个例子。相比之下，人类通常只需从几个例子或简单的指令中就能完成一项新的语言任务，而目前的NLP系统在很大程度上仍难以做到这一点。在这里，本文扩大语言模型的规模，大大改善了与任务无关的、少量的性能，有时甚至达到了与之前最先进的微调方法的竞争力。具体来说，本文训练了GPT-3，一个具有1750亿个参数的自回归语言模型，比以前的任何非稀疏语言模型多出10倍，并测试了它在少样本情况下的表现。对于所有的任务，GPT-3的应用没有任何梯度更新或微调，纯粹通过与模型的文本互动来指定任务和少量演示。GPT-3在许多NLP数据集上取得了强大的性能，包括翻译、回答问题和cloze任务，以及一些需要即时推理或领域适应的任务，如解读单词、在句子中使用一个新词或进行3位数的算术。同时，本文也发现了一些数据集，在这些数据集中，GPT-3的小样本学习仍然很困难，还有一些数据集，GPT-3面临着与大型网络语料库训练有关的方法学问题。最后，本文发现，GPT-3可以生成人类评价者难以区分的新闻文章样本。本文讨论了这一发现和GPT-3总体上的更广泛的社会影响。


## 主要解决问题/应用

GPT-3属于少样本学习语言模型，只需要少量标注数据，不管是 Zero-shot、One-shot 还是 Few-shot 都无需再进行微调。GPT-3 聚焦于更通用的 NLP 模型，主要目标是用更少的领域数据、且不经过精调步骤去解决问题。简单来说，GPT-3 是 GPT-2 的进化版，惊人的模型参数、训练数据和工作量以及结果证明了“大力出奇迹”的道理，扩展了NLP领域的想象力。

## 主要使用方法/模型

GPT-3的可学习参数达到1750亿，是之前的非稀疏语言模型的10倍以上，并在few-shot的设置上测试它的性能。对于所有子任务，GPT-3不做任何的梯度更新或者是微调。GPT-3的模型和GPT-2一样。

## 主要实验手段/数据集

除了几个常见的NLP任务，GPT-3	还在很多非常困难的任务上也有惊艳的表现。

## 创造性思考

GPT3和GPT2相比，延续了一贯的大力出奇迹的思路，继续把模型扩大了百倍以上达到了惊人的1750亿的参数级别，并且继续探索了在不对下游任务进行适配（模型结构更改和参数更新）的情况下模型的表现。

## 批判式思考

GPT-3在一些输出上也会犯一些带有偏向性的低级错误。模型规模大，训练成本高，很难根据样本进行微调。难以解释每个权重是做什么的，可解释性差；难以解释究竟是模型记住了样例还是模型学到了具体意义；只学习文本。未涉及其他模态；结构和算法的局限性。只能看当前词之前的信息（decoder）；每个词都均匀地预测下一个词，没有哪一个词更重要。

## 讨论 







