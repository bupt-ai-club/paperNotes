# Language Models are Unsupervised Multitask Learners

**作者** Alec Radford、Jeffrey Wu、Rewon Child

**发表刊物/会议：** arXiv

**发表年份：** 2019

**论文地址：** https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf

**代码地址：**

## 内容概要

自然语言处理任务，如问题回答、机器翻译、阅读理解和总结，通常在特定任务的数据集上进行监督学习。本文证明，当在一个名为WebText的数百万网页的新数据集上训练时，语言模型开始在没有任何明确监督的情况下学习这些任务。当以文档加问题为条件时，语言模型产生的答案在CoQA数据集上达到了55个F1--匹配或超过了4个基线系统中的3个，而没有使用127,000多个训练例子。语言模型的容量对zero-shot任务转移的成功至关重要，增加语言模型可以在不同的任务中以对数线性的方式提高性能。本文最大的模型GPT-2是一个1.5B参数的转化器，在zero-shot的情况下，在8个测试的语言建模数据集中的7个取得了最先进的结果，但仍然不适合WebText。该模型的样本反映了这些改进，并包含连贯的文本段落。这些发现为建立语言处理系统提供了一条有希望的道路，该系统可以从自然发生的示范中学习执行任务。

## 主要解决问题/应用

GPT2的核心思想就是认为可以用无监督的预训练模型去做有监督任务。GPT-2希望在完全不理解词的情况下建模，以便让模型可以处理任何编码的语言。GPT-2主要针对zero-shot问题。它在解决多种无监督问题时有很大提升，但是对于有监督学习则差一些。


## 主要使用方法/模型

GPT-2依然沿用GPT单向transformer的模式，只不过做了一些改进与改变。
1.	GPT-2去掉了fine-tuning层：不再针对不同任务分别进行微调建模，而是不定义这个模型应该做什么任务，模型会自动识别出来需要做什么任务。
2.	在Pretrain部分基本与GPT方法相同，在Fine-tune部分把第二阶段的Fine-tuning有监督训练具体NLP任务，换成了无监督训练具体任务，这样使得预训练和Fine-tuning的结构完全一致。
3.	增加数据集：这是一个比更大还更大的数据集。GPT-2构造了一个新数据集WebText。这些无监督样本，全部来自 Reddit的外链，而且是那些获得至少三个赞的外链，共计4500万链接。使用 Dragnet 和 Newspaper 两种工具来抽取网页内容。得到8百万网页共计 40GB 文本数据。
4.	增加网络参数：GPT-2将Transformer堆叠的层数增加到48层，隐层的维度为1600，参数量更是达到了15亿。(Bert的参数量也才只有3亿)
5.	调整transformer：将layer normalization放到每个sub-block之前，并在最后一个Self-attention后再增加一个layer normalization。


## 主要实验手段/数据集

GPT2在8个测试的语言建模数据集中的7个上表现SOTA。

## 创造性思考

GPT2最主要的贡献是探索了更大规模的模型在ZERO-SHOT的情况下的表现，没有使用任何微调，仅靠预训练+提示+预测就在8/9个任务里达到了SOTA。

## 批判式思考

LM训练大概需要三方面，模型结构，目标函数和数据。和GPT相比，GPT2模型结构和目标函数几乎是一样的，只有数据不一样，这样文章的创新就显得不是很够。模型大小是BERT large的五倍参数量，两倍的深度。训练成本高。

## 讨论 







