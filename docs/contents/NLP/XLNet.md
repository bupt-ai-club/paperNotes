# XLNet: Generalized Autoregressive Pretraining for Language Understanding

**作者** Zihang Dai、Zhilin Yang、Yiming Yang、Kristina Toutanova、Jaime CarbonellQuoc V . Le、Ruslan Salakhutdinov等

**发表刊物/会议：** NeurIPS

**发表年份：** 2019

**论文地址：**  https://arxiv.org/abs/1906.08237

**代码地址：** https://github.com/zihangdai/xlnet

## 内容概要

与基于自回归语言建模的预训练方法相比，基于去噪自编码的预训练方法（BERT等）具有双向上下文建模的能力，能取得更好的性能。然而，BERT依赖于用掩码破坏输入，忽略了掩码位置之间的依赖性，并遭受预训练-微调差异的困扰。考虑到这些优点和缺点，我们提出了一种广义自回归预训练方法XLNet，它(1)通过在所有分解顺序的排列上最大化期望似然来实现双向上下文学习;(2)通过其自回归克服了BERT的局限性。此外，XLNet将Transformer-XL(最先进的自回归模型)的思想集成到预训练中。

## 主要解决问题/应用

由于AR语言模型仅被训练为对单向上下文(向前或向后)进行编码，因此它在建模深度双向上下文时并不有效。相反，下游语言理解任务往往需要双向上下文信息。这就导致了AR语言建模和有效的预训练之间的差距。相比之下，基于AE的预训练不执行显式密度估计，而是旨在从损坏的输入中重建原始数据。然而，BERT在预训练中使用的[MASK]等人工符号在微调时真实数据不存在，导致预训练与微调之间存在差异。此外，由于预测的标记被屏蔽在输入中，BERT不能像AR语言建模中那样使用乘积规则建模联合概率。

## 主要使用方法/模型

对于AE和AR两种模型在各自的方向优点，有什么办法能构建一个模型使得同时具有AR和AE的优点并且没有它们缺点呢？这也是XLNet诞生的初衷，对于XLNet：
不再像传统AR模型中那样使用前向或者反向的固定次序作为输入，XLNet引入排列语言模型，采用排列组合的方式，每个位置的上下文可以由来自左边和右边的token组成。在期望中，每个位置都要学会利用来自所有位置的上下文信息，即，捕获双向上下文信息。
作为一个通用的AR语言模型，XLNet不再使用data corruption，即不再使用特定标识符号[MASK]。因此也就不存在BERT中的预训练和微调的不一致性。同时，自回归在分解预测tokens的联合概率时，天然地使用乘法法则，这消除了BERT中的独立性假设。
XLNet在预训练中借鉴了Transformer-XL中的segment recurrence机制的相对编码方案，其性能提升在长文本序列上尤为显著。
由于分解后次序是任意的，而target是不明确的，所以无法直接使用Transformer-XL，论文中提出采用“reparameterize the Transformer(-XL) network”以消除上述的不确定性。


## 主要实验手段/数据集

XLNet 在 20 个任务上超过了 BERT 的表现，并在 18 个任务上取得了当前最佳效果（state-of-the-art），包括机器问答、自然语言推断、情感分析和文档排序。

## 创造性思考

XLNET是基于自回模型上的，但是它不只是向前或向后，而是双方的排列来获取依赖信息，避免单向信息流。作为一种广义的AR语言模型，XLNet不依赖于数据破坏。避免mask丢失信息。避免与训练与微调的差异弊端。融合了transformerXL的方法，将片段递归机制和相关编码方案集成到与训练中。


## 批判式思考

XLNet在Transformer XL的基础上引入了随机排列和双流注意力机制，因此使得整个模型变得非常复杂。XLNet训练总共使用了126GB纯文本数据，而BERT训练只使用了13GB的数据。所以虽说最终成绩XLNet超过了BERT，但究竟是因为数据的帮助，还是模型好不得而知。

## 讨论 







